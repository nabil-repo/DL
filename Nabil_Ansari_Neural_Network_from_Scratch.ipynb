{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuWXXx832mTK"
      },
      "source": [
        "# **Neural Network from Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtuAGqKv180E"
      },
      "source": [
        "# Deep Learning Lab Assignment\n",
        "# MIT Academy of Engineering, Alandi, Pune\n",
        "# Class: BTech - Deep Learning\n",
        "\n",
        "**Name:-** Nabil Ansari \\\n",
        "**MSID:-** 75 \\\n",
        "**PRN:-** 202302040004\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objective\n",
        "This assignment involves implementing a simple feedforward neural network from scratch in Python. The focus is on understanding key neural network concepts such as forward pass, backpropagation, and gradient descent optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up1BS6a0CCGh",
        "outputId": "fb8d72de-be7e-4485-d470-5d8b37d05bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 - Loss: 0.3218585740302536\n",
            "Epoch 1000 - Loss: 0.006094780772675604\n",
            "Epoch 2000 - Loss: 0.0056595512961892394\n",
            "Epoch 3000 - Loss: 0.005592621914023347\n",
            "Epoch 4000 - Loss: 0.005561622863923562\n",
            "Epoch 5000 - Loss: 0.005539138759544439\n",
            "Epoch 6000 - Loss: 0.005521314482994205\n",
            "Epoch 7000 - Loss: 0.005507345993828759\n",
            "Epoch 8000 - Loss: 0.0054960792959482035\n",
            "Epoch 9000 - Loss: 0.005486459420232381\n",
            "\n",
            "Model Accuracy: 100.00%\n",
            "\n",
            "Enter flower measurements:\n",
            "Sepal Length (cm): 5.1\n",
            "Sepal Width (cm): 3.5\n",
            "Petal Length (cm): 1.4\n",
            "Petal Width (cm): 0.2\n",
            "\n",
            "Predicted Species: Setosa\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Activation Function\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid Activation Function: This function maps input values to the range (0,1), making it useful for neural networks dealing with probabilities.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"Derivative of Sigmoid Activation Function: This is used during backpropagation to update weights accordingly.\"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "\"\"\"\n",
        "# Neural Network Architecture\n",
        "The neural network consists of:\n",
        "- 4 input neurons (corresponding to the features in the Iris dataset)\n",
        "- 2 hidden layers with 6 and 5 neurons respectively\n",
        "- 3 output neurons (corresponding to the three classes in the dataset)\n",
        "The network uses the Sigmoid activation function for both hidden and output layers.\n",
        "\"\"\"\n",
        "\n",
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        \"\"\"Initialize network architecture with weights and biases.\"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.weights_input_hidden1 = np.random.randn(self.input_size, self.hidden_size1)\n",
        "        self.bias_hidden1 = np.random.randn(1, self.hidden_size1)\n",
        "\n",
        "        self.weights_hidden1_hidden2 = np.random.randn(self.hidden_size1, self.hidden_size2)\n",
        "        self.bias_hidden2 = np.random.randn(1, self.hidden_size2)\n",
        "\n",
        "        self.weights_hidden2_output = np.random.randn(self.hidden_size2, self.output_size)\n",
        "        self.bias_output = np.random.randn(1, self.output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass computation: Propagates input data through the network layers.\"\"\"\n",
        "        self.hidden1_input = np.dot(X, self.weights_input_hidden1) + self.bias_hidden1\n",
        "        self.hidden1_output = sigmoid(self.hidden1_input)\n",
        "\n",
        "        self.hidden2_input = np.dot(self.hidden1_output, self.weights_hidden1_hidden2) + self.bias_hidden2\n",
        "        self.hidden2_output = sigmoid(self.hidden2_input)\n",
        "\n",
        "        self.output_layer_input = np.dot(self.hidden2_output, self.weights_hidden2_output) + self.bias_output\n",
        "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"Backward pass computation: Computes errors and updates weights using gradient descent.\"\"\"\n",
        "        error_output = y - self.output_layer_output\n",
        "        output_layer_delta = error_output * sigmoid_derivative(self.output_layer_output)\n",
        "\n",
        "        error_hidden2 = output_layer_delta.dot(self.weights_hidden2_output.T)\n",
        "        hidden2_layer_delta = error_hidden2 * sigmoid_derivative(self.hidden2_output)\n",
        "\n",
        "        error_hidden1 = hidden2_layer_delta.dot(self.weights_hidden1_hidden2.T)\n",
        "        hidden1_layer_delta = error_hidden1 * sigmoid_derivative(self.hidden1_output)\n",
        "\n",
        "        self.weights_hidden2_output += self.hidden2_output.T.dot(output_layer_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        self.weights_hidden1_hidden2 += self.hidden1_output.T.dot(hidden2_layer_delta) * learning_rate\n",
        "        self.bias_hidden2 += np.sum(hidden2_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        self.weights_input_hidden1 += X.T.dot(hidden1_layer_delta) * learning_rate\n",
        "        self.bias_hidden1 += np.sum(hidden1_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"Train the network over multiple epochs using forward and backward propagation.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - self.output_layer_output))\n",
        "                print(f\"Epoch {epoch} - Loss: {loss}\")\n",
        "\n",
        "\"\"\"\n",
        "# Dataset\n",
        "The Iris dataset is used for classification. It consists of 150 samples with 4 features each, categorized into 3 different species.\n",
        "\"\"\"\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_one_hot = encoder.fit_transform(y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Neural Network\n",
        "nn = NeuralNetwork(input_size=4, hidden_size1=6, hidden_size2=5, output_size=3)\n",
        "nn.train(X_train, y_train, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "\"\"\"\n",
        "# Model Evaluation\n",
        "The trained model is evaluated on test data, and its accuracy is printed.\n",
        "\"\"\"\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = nn.forward(X_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = np.mean(predicted_labels == true_labels)\n",
        "print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\"\"\"\n",
        "# User Input for Prediction\n",
        "Allows the user to enter flower measurements and predicts the corresponding Iris species.\n",
        "\"\"\"\n",
        "\n",
        "def predict_species():\n",
        "    print(\"\\nEnter flower measurements:\")\n",
        "    sepal_length = float(input(\"Sepal Length (cm): \"))\n",
        "    sepal_width = float(input(\"Sepal Width (cm): \"))\n",
        "    petal_length = float(input(\"Petal Length (cm): \"))\n",
        "    petal_width = float(input(\"Petal Width (cm): \"))\n",
        "\n",
        "    user_input = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
        "    user_input_scaled = scaler.transform(user_input)\n",
        "\n",
        "    prediction = nn.forward(user_input_scaled)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    species = [\"Setosa\", \"Versicolor\", \"Virginica\"]\n",
        "    print(f\"\\nPredicted Species: {species[predicted_class]}\")\n",
        "\n",
        "predict_species()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Declaration:\n",
        "I, Nabil Ansari, confirm that the work submitted in this assignment is my own and has been completed\n",
        "following academic integrity guidelines. The code is uploaded on my GitHub repository account, and the\n",
        "repository link is provided below:\n",
        "# GitHub Repository Link: https://github.com/nabil-repo/DL\n",
        "# Signature:Nabil Aman Aasif Ahmad Ansari"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
